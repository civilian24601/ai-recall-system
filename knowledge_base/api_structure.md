# ğŸŒ API Structure - AI Recall System

## **ğŸ“Œ Overview**

This document outlines the API endpoints that power the **AI Recall System**.  
The API serves as a bridge between **LM Studio, ChromaDB, and AI agents**, enabling efficient model execution and knowledge retrieval.

âœ… **Backend:** Flask API  
âœ… **Primary Model Execution:** LM Studio (local)  
âœ… **Knowledge Retrieval:** ChromaDB  
âœ… **OS Compatibility:** Supports **Windows & WSL**  

---

## **ğŸ“‚ API Script: `api_structure.py`**

ğŸ“ **Location:** `/mnt/f/projects/ai-recall-system/code_base/api_structure.py`

ğŸ’¡ **Purpose:**  

- Handles **requests to the local AI models running via LM Studio**  
- Provides endpoints for **retrieving stored knowledge from ChromaDB**  
- Supports **multi-agent workflows & debugging recall**  

---

## Updated Section: Optional API for QA or External Integrations

`api_structure.py` defines a simple Flask app exposing `/api/task`. 
- This is especially helpful for manual testing or hooking up a small UI/CLI that wants to query the local LM Studio.
- In parallel, `agent_manager.py` can also connect directly to LM Studio for behind-the-scenes automation.

### Environment Detection

We now unify environment detection (e.g., WSL vs. native Windows) via a shared utility function in `network_utils.py`. This avoids duplication and ensures consistency when referencing LM Studioâ€™s port/IP address.


## **ğŸ”¹ API URL Detection (Windows & WSL Compatibility)**

The API must handle **Windows & WSL environments** seamlessly.  
The following function **auto-detects the correct API URL**:

```python
def detect_api_url():
    """Detect the correct API URL based on whether we are in WSL or native Windows."""
    wsl_ip = "172.17.128.1"
    default_url = "http://localhost:1234/v1/chat/completions"

    try:
        with open("/proc/version", "r") as f:
            if "microsoft" in f.read().lower():
                print(f"ğŸ”¹ Detected WSL! Using Windows IP: {wsl_ip}")
                return f"http://{wsl_ip}:1234/v1/chat/completions"
    except FileNotFoundError:
        pass

    print(f"ğŸ”¹ Using default API URL: {default_url}")
    return default_url

âœ… Ensures stable AI model interaction across OS environments.

ğŸ“Œ API Endpoints
1ï¸âƒ£ /query/model â†’ Execute AI Model (via LM Studio)
ğŸ”¹ Description: Sends a request to LM Studio for AI-generated responses.
ğŸ”¹ Method: POST
ğŸ”¹ Expected Input:

{
    "model": "deepseek-coder-33b-instruct",
    "prompt": "Explain recursion in Python.",
    "temperature": 0.7
}
ğŸ”¹ Example Response:

{
    "response": "Recursion is a method where the function calls itself..."
}
âœ… Supports multiple models (depending on whatâ€™s loaded in LM Studio).
âœ… Handles different temperature settings for response randomness.

2ï¸âƒ£ /query/knowledge â†’ Retrieve Stored Knowledge (ChromaDB)
ğŸ”¹ Description: Queries ChromaDB for past work, debugging logs, or relevant project knowledge.
ğŸ”¹ Method: POST
ğŸ”¹ Expected Input:

{
    "query": "What debugging steps did we follow for the last API failure?"
}
ğŸ”¹ Example Response:

{
    "retrieved_knowledge": "The last API failure was related to a missing API key. Debugging steps included..."
}
âœ… Allows AI agents to retrieve previous work for better debugging & recall.
âœ… Ensures that AI doesnâ€™t suggest redundant fixes.

3ï¸âƒ£ /query/codebase â†’ Retrieve Code Snippets
ğŸ”¹ Description: Searches the indexed project codebase for relevant functions or classes.
ğŸ”¹ Method: POST
ğŸ”¹ Expected Input:


{
    "query": "How do we handle API authentication?",
    "language": "python"
}
ğŸ”¹ Example Response:

json
Copy
Edit
{
    "matches": [
        {
            "filename": "auth_handler.py",
            "snippet": "def authenticate_user(api_key): ..."
        }
    ]
}
âœ… Allows AI to reference past implementations instead of regenerating from scratch.
âœ… Helps maintain coding consistency across projects.

ğŸ“Œ API Deployment & Testing
ğŸ“Œ To start the API server:

python3 /mnt/f/projects/ai-recall-system/code_base/api_structure.py
ğŸ“Œ To test if the API is running (from CLI):


curl -X POST http://localhost:5000/query/model -H "Content-Type: application/json" -d '{"model":"deepseek-coder-33b-instruct", "prompt":"Explain recursion in Python."}'
ğŸ“Œ To test from a Python script:

import requests

url = "http://localhost:5000/query/model"
payload = {
    "model": "deepseek-coder-33b-instruct",
    "prompt": "Explain recursion in Python."
}
response = requests.post(url, json=payload)
print(response.json())
âœ… Ensures the API correctly interacts with LM Studio.
âœ… Can be tested easily from CLI or Python scripts.

ğŸ“Œ Summary
ğŸ“Œ This API structure ensures:
âœ… Stable local AI execution via Flask API â†’ LM Studio
âœ… Windows/WSL compatibility for seamless agent workflows
âœ… Direct access to knowledge recall & debugging history via ChromaDB
âœ… Modular endpoints for code retrieval, knowledge recall, and model execution

ğŸ“… Last Updated: February 2025
ğŸ”¹ Maintained by AI Recall System
